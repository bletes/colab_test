{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35416bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "                                   result  table  \\\n",
      "date                                               \n",
      "2023-03-28 09:27:52.473740+00:00  _result    0.0   \n",
      "2023-03-28 09:27:53.473740+00:00      NaN    NaN   \n",
      "2023-03-28 09:27:54.473740+00:00      NaN    NaN   \n",
      "2023-03-28 09:27:55.473740+00:00      NaN    NaN   \n",
      "2023-03-28 09:27:56.473740+00:00      NaN    NaN   \n",
      "...                                   ...    ...   \n",
      "2023-04-20 07:01:54.473740+00:00      NaN    NaN   \n",
      "2023-04-20 07:01:55.473740+00:00      NaN    NaN   \n",
      "2023-04-20 07:01:56.473740+00:00      NaN    NaN   \n",
      "2023-04-20 07:01:57.473740+00:00      NaN    NaN   \n",
      "2023-04-20 07:01:58.473740+00:00      NaN    NaN   \n",
      "\n",
      "                                                            _time  \\\n",
      "date                                                                \n",
      "2023-03-28 09:27:52.473740+00:00 2023-03-28 09:27:52.473740+00:00   \n",
      "2023-03-28 09:27:53.473740+00:00                              NaT   \n",
      "2023-03-28 09:27:54.473740+00:00                              NaT   \n",
      "2023-03-28 09:27:55.473740+00:00                              NaT   \n",
      "2023-03-28 09:27:56.473740+00:00                              NaT   \n",
      "...                                                           ...   \n",
      "2023-04-20 07:01:54.473740+00:00                              NaT   \n",
      "2023-04-20 07:01:55.473740+00:00                              NaT   \n",
      "2023-04-20 07:01:56.473740+00:00                              NaT   \n",
      "2023-04-20 07:01:57.473740+00:00                              NaT   \n",
      "2023-04-20 07:01:58.473740+00:00                              NaT   \n",
      "\n",
      "                                  _niv_temperature_A  _niv_temperature_B  \\\n",
      "date                                                                       \n",
      "2023-03-28 09:27:52.473740+00:00           -1.912752           19.980537   \n",
      "2023-03-28 09:27:53.473740+00:00           -1.903075           20.070179   \n",
      "2023-03-28 09:27:54.473740+00:00           -1.904603           20.090552   \n",
      "2023-03-28 09:27:55.473740+00:00           -1.903585           20.026377   \n",
      "2023-03-28 09:27:56.473740+00:00           -1.890342           20.024339   \n",
      "...                                              ...                 ...   \n",
      "2023-04-20 07:01:54.473740+00:00          260.904557          283.494550   \n",
      "2023-04-20 07:01:55.473740+00:00          260.826631          283.413566   \n",
      "2023-04-20 07:01:56.473740+00:00          260.748706          283.415604   \n",
      "2023-04-20 07:01:57.473740+00:00          260.742594          283.366199   \n",
      "2023-04-20 07:01:58.473740+00:00          260.748196          283.276047   \n",
      "\n",
      "                                   _power_A  _power_B  \\\n",
      "date                                                    \n",
      "2023-03-28 09:27:52.473740+00:00   3.067392  3.138074   \n",
      "2023-03-28 09:27:53.473740+00:00   3.066394  3.138074   \n",
      "2023-03-28 09:27:54.473740+00:00   3.066394  3.138074   \n",
      "2023-03-28 09:27:55.473740+00:00   3.066394  3.138074   \n",
      "2023-03-28 09:27:56.473740+00:00   3.066394  3.138074   \n",
      "...                                     ...       ...   \n",
      "2023-04-20 07:01:54.473740+00:00  26.653256  6.678081   \n",
      "2023-04-20 07:01:55.473740+00:00  26.650840  6.677859   \n",
      "2023-04-20 07:01:56.473740+00:00  26.652404  6.677859   \n",
      "2023-04-20 07:01:57.473740+00:00  26.653968  6.677859   \n",
      "2023-04-20 07:01:58.473740+00:00  26.653968  6.677859   \n",
      "\n",
      "                                                             date  \n",
      "date                                                               \n",
      "2023-03-28 09:27:52.473740+00:00 2023-03-28 09:27:52.473740+00:00  \n",
      "2023-03-28 09:27:53.473740+00:00 2023-03-28 09:27:53.473740+00:00  \n",
      "2023-03-28 09:27:54.473740+00:00 2023-03-28 09:27:54.473740+00:00  \n",
      "2023-03-28 09:27:55.473740+00:00 2023-03-28 09:27:55.473740+00:00  \n",
      "2023-03-28 09:27:56.473740+00:00 2023-03-28 09:27:56.473740+00:00  \n",
      "...                                                           ...  \n",
      "2023-04-20 07:01:54.473740+00:00 2023-04-20 07:01:54.473740+00:00  \n",
      "2023-04-20 07:01:55.473740+00:00 2023-04-20 07:01:55.473740+00:00  \n",
      "2023-04-20 07:01:56.473740+00:00 2023-04-20 07:01:56.473740+00:00  \n",
      "2023-04-20 07:01:57.473740+00:00 2023-04-20 07:01:57.473740+00:00  \n",
      "2023-04-20 07:01:58.473740+00:00 2023-04-20 07:01:58.473740+00:00  \n",
      "\n",
      "[1978447 rows x 8 columns]\n",
      "0           -1.912752\n",
      "1           19.980537\n",
      "2            3.067392\n",
      "3            3.138074\n",
      "4           -1.903075\n",
      "              ...    \n",
      "7913783      6.677859\n",
      "7913784    260.748196\n",
      "7913785    283.276047\n",
      "7913786     26.653968\n",
      "7913787      6.677859\n",
      "Name: close, Length: 7913788, dtype: float64\n",
      "Split data into training set and test set... Number of training samples/ test samples: 5302238 2611550\n",
      "[[0.00038071]\n",
      " [0.05953203]\n",
      " [0.01383607]\n",
      " [0.01402704]\n",
      " [0.00040686]\n",
      " [0.05977423]\n",
      " [0.01383338]\n",
      " [0.01402704]\n",
      " [0.00040273]\n",
      " [0.05982926]]\n",
      "[[0.01383338]\n",
      " [0.01402704]]\n",
      "[[0.01899783]\n",
      " [0.03210416]\n",
      " [0.21032828]\n",
      " ...\n",
      " [0.77090424]\n",
      " [0.07756232]\n",
      " [0.02359084]]\n",
      "Epoch 1/10\n",
      "41424/41424 [==============================] - 220s 5ms/step - loss: 3.9192e-04\n",
      "Epoch 2/10\n",
      "41424/41424 [==============================] - 221s 5ms/step - loss: 9.7731e-06\n",
      "Epoch 3/10\n",
      "41424/41424 [==============================] - 218s 5ms/step - loss: 6.0438e-06\n",
      "Epoch 4/10\n",
      "41424/41424 [==============================] - 218s 5ms/step - loss: 5.4445e-06\n",
      "Epoch 5/10\n",
      "41424/41424 [==============================] - 218s 5ms/step - loss: 5.1189e-06\n",
      "Epoch 6/10\n",
      "41424/41424 [==============================] - 218s 5ms/step - loss: 4.8708e-06\n",
      "Epoch 7/10\n",
      "41424/41424 [==============================] - 220s 5ms/step - loss: 4.7212e-06\n",
      "Epoch 8/10\n",
      "41424/41424 [==============================] - 222s 5ms/step - loss: 4.6167e-06\n",
      "Epoch 9/10\n",
      "41424/41424 [==============================] - 219s 5ms/step - loss: 4.5440e-06\n",
      "Epoch 10/10\n",
      "41424/41424 [==============================] - 222s 5ms/step - loss: 4.4883e-06\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 4)                 96        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106\n",
      "Trainable params: 106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "41424/41424 [==============================] - 57s 1ms/step\n",
      "20403/20403 [==============================] - 28s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27360\\3011847324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;31m# invert predictions and targets to unscaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[0mtrainPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainPredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m \u001b[0mtrainY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[0mtestPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[0mtestY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         X = check_array(\n\u001b[0m\u001b[0;32m    526\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         )\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    792\u001b[0m                 ) from e\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    795\u001b[0m                 \u001b[1;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                 \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 4. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# Import a Client\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from influxdb_client import InfluxDBClient\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# parameters to be set (\"optimum\" hyperparameters obtained from grid search):\n",
    "look_back = 38#移动平均长度\n",
    "epochs = 10#迭代多少次\n",
    "batch_size = 32#默认值\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# read data from InfluxDB 2.0 \n",
    "token = \"K2TFepmH-x3a0wHiponbMFvPgg7r3BQv63ET1YqZk4glJOIHunvrw5Yjkj3XxxUaxuIsssjRfO9VA6YclLizEA==\"\n",
    "org = \"CYTEK\"\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, org=org, debug=False)\n",
    "query_api = client.query_api()\n",
    "data_influxdb = query_api.query_data_frame('from(bucket:\"Xiaoluzi\")'\n",
    "                                        '|> range(start:2023-03-28T01:44:00Z, stop:2023-04-20T07:02:00Z)'\n",
    "                                        '|> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")'\n",
    "                                        '|> keep(columns: [\"_power_A\",\"_niv_temperature_A\",\"_power_B\",\"_niv_temperature_B\",\"_time\"])')\n",
    "\n",
    "#delete niv==0\n",
    "data_influxdb=data_influxdb[~data_influxdb['_niv_temperature_A'].isin([0.000])]\n",
    "\n",
    "#interpol 1s\n",
    "temp_data=data_influxdb\n",
    "temp_data.index=pd.to_datetime(temp_data['_time'],unit='s')\n",
    "interpol=interpolate.interp1d(temp_data['_time'], temp_data[\"_niv_temperature_A\"], kind='linear')\n",
    "temp_data['date']=temp_data.index\n",
    "helper = pd.DataFrame({'date': pd.date_range(temp_data.index.min(), temp_data.index.max(),freq='1S')})\n",
    "temp_data2= pd.merge(temp_data, helper, on='date', how='outer').sort_values('date')\n",
    "temp_data2.index=temp_data2['date']\n",
    "temp_data2[\"_niv_temperature_A\"] = temp_data2[\"_niv_temperature_A\"].interpolate(method='linear')\n",
    "temp_data2[\"_niv_temperature_B\"] = temp_data2[\"_niv_temperature_B\"].interpolate(method='linear')\n",
    "temp_data2[\"_power_A\"] = temp_data2[\"_power_A\"].interpolate(method='linear')\n",
    "temp_data2[\"_power_B\"] = temp_data2[\"_power_B\"].interpolate(method='linear')\n",
    "temp_data2=temp_data2[temp_data2['date'].isin(helper['date'])]\n",
    "data_influxdb=temp_data2\n",
    "print(data_influxdb)\n",
    "\n",
    "\n",
    "train_data=pd.DataFrame()\n",
    "train_data['P_and_T_data']=np.reshape(np.array(data_influxdb[['_niv_temperature_A',\"_niv_temperature_B\",'_power_A',\"_power_B\"]]),len(data_influxdb['_niv_temperature_A'])*4)\n",
    "\n",
    "print(train_data['P_and_T_data'])\n",
    "\n",
    "# save train_data values as type of floating point number\n",
    "train_data_tofloat = train_data.P_and_T_data.values.astype('float32')\n",
    "\n",
    "# reshape to column vector\n",
    "train_data_tofloat = train_data_tofloat.reshape(len(train_data_tofloat), 1)\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_tofloat = scaler.fit_transform(train_data_tofloat)\n",
    "\n",
    "# split data into training set and test set\n",
    "train_size = int(len(train_data_tofloat) * 0.67)#2/3用作训练集\n",
    "if train_size%2==1:\n",
    "\ttrain_size+=1\n",
    "test_size = len(train_data_tofloat) - train_size\n",
    "train, test = train_data_tofloat[0:train_size,:], train_data_tofloat[train_size:len(train_data_tofloat),:]\n",
    "\n",
    "print('Split data into training set and test set... Number of training samples/ test samples:', len(train), len(test))\n",
    "# convert an array of values into a time series dataset \n",
    "# in form \n",
    "#                     X                     Y\n",
    "# t-look_back+1, t-look_back+2, ..., t     t+1\n",
    "#print(train.head(10))\n",
    "print(train[0:10])\n",
    "print(train[10:12])\n",
    "print(test)\n",
    "def create_dataset(dataset, look_back):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\tif i%4==0:\n",
    "\t\t\tdataX.append(dataset[i:(i+look_back), 0])\n",
    "\t\t\tdataY.append(dataset[i+look_back:(i+look_back+2), 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "# convert train_data into time series dataset\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input of the LSTM to be format [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "trainY = np.reshape(trainY, (trainY.shape[0], trainY.shape[1], 1))\n",
    "testY = np.reshape(testY, (testY.shape[0], testY.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1)))#4 :Positive integer, dimensionality of the output space.\n",
    "model.add(Dense(2),activation='sigmoid')#神经网络层\n",
    "model.compile(loss='mse', optimizer='adam')#MeanSquaredErro #loss y(true)-y(expect)\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size)# batch_size：Number of samples per gradient update.\n",
    "model.summary()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions and targets to unscaled\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift predictions of training data for plotting\n",
    "te=[]\n",
    "for i in range(len(train_data_tofloat)):\n",
    "\tif i%2==0:\n",
    "\t\tte.append(train_data_tofloat[i])\n",
    "train_data_tofloat=te\n",
    "trainPredictPlot = np.empty_like(train_data_tofloat)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift predictions of test data for plotting\n",
    "testPredictPlot = np.empty_like(train_data_tofloat)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+1:len(train_data_tofloat)-1, :] = testPredict[:]\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(train_data_tofloat))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ebafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\Administrator\\Desktop\\tfmodel\\TABPAB38-03-28-04-20-2023-2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
